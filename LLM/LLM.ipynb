{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import os\n",
    "from spacy.training.example import Example\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('FIR_DATASET(updated).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  #\n",
    "        \n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Description'] = data['Description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Description'].tolist()  \n",
    "y = data['section'].tolist()  \n",
    "\n",
    "print(\"Input (X) sample:\")\n",
    "print(X[:5])\n",
    "print(\"\\nOutput (y) sample:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "textcat = nlp.add_pipe(\"textcat_multilabel\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for label in set(y):\n",
    "    textcat.add_label(str(label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X, [{\"cats\": {label: (section == label) for label in set(y)}} for section in y]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_exceptions = [\"textcat_multilabel\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(10):  #\n",
    "        losses = {}\n",
    "        for texts, annotations in train_data:\n",
    "            \n",
    "            text = ' '.join(texts)\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "            \n",
    "            \n",
    "            nlp.update([example], drop=0.5, losses=losses, sgd=optimizer, exclude=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "            \n",
    "        print(\"Epoch:\", epoch, \"Loss:\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_sections(complaint_text, nlp_model, section_labels, data):\n",
    "    processed_text = preprocess_text(complaint_text)\n",
    "    processed_text = ' '.join(processed_text)\n",
    "\n",
    "    \n",
    "    similarities = []\n",
    "    for _, row in data.iterrows():\n",
    "        row_text = ' '.join(preprocess_text(row['Description']))\n",
    "        similarity = nlp_model(processed_text).similarity(nlp_model(row_text))\n",
    "        similarities.append(similarity)\n",
    "\n",
    "   \n",
    "    max_similarity_index = similarities.index(max(similarities))\n",
    "    suggested_section = data.iloc[max_similarity_index]['section']\n",
    "\n",
    "    return suggested_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "complaint_description = \"The suspect stole my wallet and assaulted me.\"\n",
    "suggested_section = suggest_sections(complaint_description, nlp, textcat.labels, data)\n",
    "print(\"Suggested Section:\", suggested_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fir():\n",
    "    # Input from the user\n",
    "    complainant_name = input(\"Enter complainant's name: \")\n",
    "    father_name = input(\"Enter father's/husband's name: \")\n",
    "    address = input(\"Enter address: \")\n",
    "    phone_number = input(\"Enter phone number and fax: \")\n",
    "    email = input(\"Enter email: \")\n",
    "    place_of_occurrence = input(\"Enter place of occurrence: \")\n",
    "    date_of_occurrence = input(\"Enter date and hour of occurrence: \")\n",
    "\n",
    "    # Fetch suggestions from the model based on the complaint description\n",
    "    complaint_description = input(\"Enter the complaint description: \")\n",
    "    suggested_section = suggest_sections(complaint_description, nlp, textcat.labels, data)\n",
    "\n",
    "    # Retrieve additional information from the dataset based on the identified section\n",
    "    section_info = data[data['section'] == suggested_section].iloc[0]\n",
    "    bailable = section_info['Bailable']\n",
    "    cognizable = section_info['Cognizable']\n",
    "    court = section_info['Court']\n",
    "    punishment = section_info['Punishment']\n",
    "    offense_nature = section_info['Offense']  # Automatically fill the nature of the offense\n",
    "\n",
    "    property_description = input(\"Enter particulars of the property: \")\n",
    "    accused_description = input(\"Enter description of the accused: \")\n",
    "    witness_details = input(\"Enter details of witnesses (if any): \")\n",
    "    complaint = input(\"Enter complaint: \")\n",
    "\n",
    "    # Create the FIR template\n",
    "    fir_template = f\"\"\"Police Station: [Police Station]\n",
    "District: [District]\n",
    "\n",
    "1. Personal details of the Complainant / Informant:\n",
    "(a) Name: {complainant_name}\n",
    "(b) Father's / Husband's Name: {father_name}\n",
    "(c) Address: {address}\n",
    "(d) Phone number & Fax: {phone_number}\n",
    "(e) Email: {email}\n",
    "\n",
    "2. Place of Occurrence: {place_of_occurrence}\n",
    "\n",
    "3. Date and Hour of Occurrence: {date_of_occurrence}\n",
    "\n",
    "4. Offence:\n",
    "(a) Nature of the offence: {offense_nature}  \n",
    "(b) Section: {suggested_section}  \n",
    "(c) Particulars of the property: {property_description}\n",
    "\n",
    "5. Description of the accused: {accused_description} \n",
    "\n",
    "6. Additional Section Information:\n",
    "   - Bailable: {bailable}\n",
    "   - Cognizable: {cognizable}\n",
    "   - Court: {court}\n",
    "   - Punishment: {punishment}\n",
    "\n",
    "7. Details of witnesses (if any): {witness_details}\n",
    "\n",
    "8. Complaint: {complaint}\n",
    "\"\"\"\n",
    "\n",
    "    # Return the generated FIR\n",
    "    return fir_template\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "fir_text = generate_fir()\n",
    "print(fir_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
