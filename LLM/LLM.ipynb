{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import os\n",
    "from spacy.training.example import Example\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rebel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('FIR_DATASET(updated).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is not null\n",
    "        # Remove special characters and convert to lowercase\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Description'] = data['Description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URL  \\\n",
      "0  https://lawrato.com/indian-kanoon/ipc/section-140   \n",
      "1  https://lawrato.com/indian-kanoon/ipc/section-127   \n",
      "2  https://lawrato.com/indian-kanoon/ipc/section-128   \n",
      "3  https://lawrato.com/indian-kanoon/ipc/section-129   \n",
      "4  https://lawrato.com/indian-kanoon/ipc/section-130   \n",
      "\n",
      "                                         Description  \\\n",
      "0  [description, of, ipc, section, 140, according...   \n",
      "1  [description, of, ipc, section, 127, according...   \n",
      "2  [description, of, ipc, section, 128, according...   \n",
      "3  [description, of, ipc, section, 129, according...   \n",
      "4  [description, of, ipc, section, 130, according...   \n",
      "\n",
      "                                             Offense  \\\n",
      "0  Wearing the dress or carrying any token used b...   \n",
      "1  Receiving property taken by war or depredation...   \n",
      "2  Public servant voluntarily allowing prisoner o...   \n",
      "3  Public servant negligently suffering prisoner ...   \n",
      "4  Aiding escape of, rescuing or harbouring, such...   \n",
      "\n",
      "                                 Punishment  Cognizable      Bailable  \\\n",
      "0                  3 Months or Fine or Both  Cognizable      Bailable   \n",
      "1   7 Years + Fine + forfeiture of property  Cognizable  Non-Bailable   \n",
      "2  Imprisonment for Life or 10 Years + Fine  Cognizable  Non-Bailable   \n",
      "3        Simple Imprisonment 3 Years + Fine  Cognizable      Bailable   \n",
      "4  Imprisonment for Life or 10 Years + Fine  Cognizable  Non-Bailable   \n",
      "\n",
      "                    Court section  \n",
      "0          Any Magistrate     140  \n",
      "1        Court of Session     127  \n",
      "2        Court of Session     128  \n",
      "3  Magistrate First Class     129  \n",
      "4        Court of Session     130  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (X) sample:\n",
      "[['description', 'of', 'ipc', 'section', '140', 'according', 'to', 'section', '140', 'of', 'indian', 'penal', 'code', 'whoever', 'not', 'being', 'a', 'soldier', 'sailor', 'or', 'airman', 'in', 'the', 'military', 'naval', 'or', 'air', 'service', 'of', 'the', 'government', 'of', 'india', 'wears', 'any', 'garb', 'or', 'carries', 'any', 'token', 'resembling', 'any', 'garb', 'or', 'token', 'used', 'by', 'such', 'a', 'soldier', 'sailor', 'or', 'airman', 'with', 'the', 'intention', 'that', 'it', 'may', 'be', 'believed', 'that', 'he', 'is', 'such', 'a', 'soldier', 'sailor', 'or', 'airman', 'shall', 'be', 'punished', 'with', 'imprisonment', 'of', 'either', 'description', 'for', 'a', 'term', 'which', 'may', 'extend', 'to', 'three', 'months', 'or', 'with', 'fine', 'which', 'may', 'extend', 'to', 'five', 'hundred', 'rupees', 'or', 'with', 'both', 'ipc', '140', 'in', 'simple', 'words', 'if', 'someone', 'who', 'is', 'not', 'a', 'military', 'member', 'wears', 'a', 'uniform', 'or', 'carries', 'something', 'resembling', 'a', 'military', 'uniform', 'to', 'deceive', 'others', 'into', 'believing', 'they', 'are', 'a', 'soldier', 'sailor', 'or', 'airman', 'they', 'can', 'be', 'punished', 'with', 'up', 'to', 'three', 'months', 'in', 'jail', 'a', 'fine', 'of', 'up', 'to', 'five', 'hundred', 'rupees', 'or', 'both'], ['description', 'of', 'ipc', 'section', '127', 'according', 'to', 'section', '127', 'of', 'indian', 'penal', 'code', 'whoever', 'receives', 'any', 'property', 'knowing', 'the', 'same', 'to', 'have', 'been', 'taken', 'in', 'the', 'commission', 'of', 'any', 'of', 'the', 'offences', 'mentioned', 'in', 'sections', '125', 'and', '126', 'shall', 'be', 'punished', 'with', 'imprisonment', 'of', 'either', 'description', 'for', 'a', 'term', 'which', 'may', 'extend', 'to', 'seven', 'years', 'and', 'shall', 'also', 'be', 'liable', 'to', 'fine', 'and', 'to', 'forfeiture', 'of', 'the', 'property', 'so', 'received', 'ipc', '127', 'in', 'simple', 'words', 'if', 'someone', 'receives', 'property', 'knowing', 'it', 'was', 'taken', 'during', 'the', 'commission', 'of', 'certain', 'offenses', 'mentioned', 'in', 'sections', '125', 'and', '126', 'they', 'can', 'be', 'punished', 'with', 'imprisonment', 'of', 'up', 'to', 'seven', 'years', 'fined', 'and', 'the', 'property', 'can', 'be', 'forfeited'], ['description', 'of', 'ipc', 'section', '128', 'according', 'to', 'section', '128', 'of', 'indian', 'penal', 'code', 'whoever', 'being', 'a', 'public', 'servant', 'and', 'having', 'the', 'custody', 'of', 'any', 'state', 'prisoner', 'or', 'prisoner', 'of', 'war', 'voluntarily', 'allows', 'such', 'prisoner', 'to', 'escape', 'from', 'any', 'place', 'in', 'which', 'such', 'prisoner', 'is', 'confined', 'shall', 'be', 'punished', 'with', 'imprisonment', 'for', 'life', 'or', 'imprisonment', 'of', 'either', 'description', 'for', 'a', 'term', 'which', 'may', 'extend', 'to', 'ten', 'years', 'and', 'shall', 'also', 'be', 'liable', 'to', 'fine', 'ipc', '128', 'in', 'simple', 'words', 'if', 'a', 'public', 'servant', 'responsible', 'for', 'the', 'custody', 'of', 'a', 'state', 'prisoner', 'or', 'prisoner', 'of', 'war', 'intentionally', 'allows', 'the', 'prisoner', 'to', 'escape', 'from', 'confinement', 'they', 'can', 'be', 'punished', 'with', 'life', 'imprisonment', 'or', 'imprisonment', 'of', 'up', 'to', 'ten', 'years', 'and', 'may', 'also', 'be', 'fined'], ['description', 'of', 'ipc', 'section', '129', 'according', 'to', 'section', '129', 'of', 'indian', 'penal', 'code', 'whoever', 'being', 'a', 'public', 'servant', 'and', 'having', 'the', 'custody', 'of', 'any', 'state', 'prisoner', 'or', 'prisoner', 'of', 'war', 'negligently', 'suffers', 'such', 'prisoner', 'to', 'escape', 'from', 'any', 'place', 'of', 'confinement', 'in', 'which', 'such', 'prisoner', 'is', 'confined', 'shall', 'be', 'punished', 'with', 'simple', 'imprisonment', 'for', 'a', 'term', 'which', 'may', 'extend', 'to', 'three', 'years', 'and', 'shall', 'also', 'be', 'liable', 'to', 'fine', 'ipc', '129', 'in', 'simple', 'words', 'if', 'a', 'public', 'servant', 'responsible', 'for', 'the', 'custody', 'of', 'a', 'state', 'prisoner', 'or', 'prisoner', 'of', 'war', 'carelessly', 'allows', 'the', 'prisoner', 'to', 'escape', 'from', 'confinement', 'they', 'can', 'be', 'punished', 'with', 'up', 'to', 'three', 'years', 'in', 'jail', 'and', 'may', 'also', 'be', 'fined'], ['description', 'of', 'ipc', 'section', '130', 'according', 'to', 'section', '130', 'of', 'indian', 'penal', 'code', 'whoever', 'knowingly', 'aids', 'or', 'assists', 'any', 'state', 'prisoner', 'or', 'prisoner', 'of', 'war', 'in', 'escaping', 'from', 'lawful', 'custody', 'or', 'rescues', 'or', 'attempts', 'to', 'rescue', 'any', 'such', 'prisoner', 'or', 'harbours', 'or', 'conceals', 'any', 'such', 'prisoner', 'who', 'has', 'escaped', 'from', 'lawful', 'custody', 'or', 'offers', 'or', 'attempts', 'to', 'offer', 'any', 'resistance', 'to', 'the', 'recapture', 'of', 'such', 'prisoner', 'shall', 'be', 'punished', 'with', 'imprisonment', 'for', 'life', 'or', 'with', 'imprisonment', 'of', 'either', 'description', 'for', 'a', 'term', 'which', 'may', 'extend', 'to', 'ten', 'years', 'and', 'shall', 'also', 'be', 'liable', 'to', 'fine', 'ipc', '130', 'in', 'simple', 'words', 'if', 'someone', 'knowingly', 'helps', 'a', 'state', 'prisoner', 'or', 'prisoner', 'of', 'war', 'escape', 'from', 'lawful', 'custody', 'rescues', 'them', 'hides', 'them', 'after', 'their', 'escape', 'or', 'resists', 'their', 'recapture', 'they', 'can', 'be', 'punished', 'with', 'life', 'imprisonment', 'or', 'imprisonment', 'of', 'up', 'to', 'ten', 'years', 'and', 'may', 'also', 'be', 'fined']]\n",
      "\n",
      "Output (y) sample:\n",
      "['140', '127', '128', '129', '130']\n"
     ]
    }
   ],
   "source": [
    "X = data['Description'].tolist()  # Input - Complainant-provided information\n",
    "y = data['section'].tolist()  # Output - Relevant FIR section\n",
    "\n",
    "print(\"Input (X) sample:\")\n",
    "print(X[:5])\n",
    "print(\"\\nOutput (y) sample:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the pre-trained spaCy model\n",
    "textcat = nlp.add_pipe(\"textcat_multilabel\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for label in set(y):\n",
    "    textcat.add_label(str(label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X, [{\"cats\": {label: (section == label) for label in set(y)}} for section in y]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_exceptions = [\"textcat_multilabel\", \"tagger\", \"parser\", \"ner\", \"lemmatizer\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: {'textcat_multilabel': 2.0126108452677727}\n",
      "Epoch: 1 Loss: {'textcat_multilabel': 1.0018158110324293}\n",
      "Epoch: 2 Loss: {'textcat_multilabel': 1.0013745443429798}\n",
      "Epoch: 3 Loss: {'textcat_multilabel': 0.9954855925752781}\n",
      "Epoch: 4 Loss: {'textcat_multilabel': 0.9543059317256848}\n",
      "Epoch: 5 Loss: {'textcat_multilabel': 0.9158862404333377}\n",
      "Epoch: 6 Loss: {'textcat_multilabel': 0.884065496340213}\n",
      "Epoch: 7 Loss: {'textcat_multilabel': 0.8413868785200407}\n",
      "Epoch: 8 Loss: {'textcat_multilabel': 0.784616856372395}\n",
      "Epoch: 9 Loss: {'textcat_multilabel': 0.7129308046686447}\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(10):  #\n",
    "        losses = {}\n",
    "        for texts, annotations in train_data:\n",
    "            \n",
    "            text = ' '.join(texts)\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "            \n",
    "            \n",
    "            nlp.update([example], drop=0.5, losses=losses, sgd=optimizer, exclude=[\"tagger\", \"parser\", \"ner\", \"textcat\"])\n",
    "            \n",
    "        print(\"Epoch:\", epoch, \"Loss:\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_sections(complaint_text, nlp_model, section_labels, data):\n",
    "    processed_text = preprocess_text(complaint_text)\n",
    "    processed_text = ' '.join(processed_text)\n",
    "\n",
    "    \n",
    "    similarities = []\n",
    "    for _, row in data.iterrows():\n",
    "        row_text = ' '.join(preprocess_text(row['Description']))\n",
    "        similarity = nlp_model(processed_text).similarity(nlp_model(row_text))\n",
    "        similarities.append(similarity)\n",
    "\n",
    "   \n",
    "    max_similarity_index = similarities.index(max(similarities))\n",
    "    suggested_section = data.iloc[max_similarity_index]['section']\n",
    "\n",
    "    return suggested_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115048/536671001.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = nlp_model(processed_text).similarity(nlp_model(row_text))\n",
      "/tmp/ipykernel_115048/536671001.py:9: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = nlp_model(processed_text).similarity(nlp_model(row_text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Section: 140\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "complaint_description = \"The suspect stole my wallet and assaulted me.\"\n",
    "suggested_section = suggest_sections(complaint_description, nlp, textcat.labels, data)\n",
    "print(\"Suggested Section:\", suggested_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fir():\n",
    "   \n",
    "    complainant_name = input(\"Enter complainant's name: \")\n",
    "    father_name = input(\"Enter father's/husband's name: \")\n",
    "    address = input(\"Enter address: \")\n",
    "    phone_number = input(\"Enter phone number and fax: \")\n",
    "    email = input(\"Enter email: \")\n",
    "    place_of_occurrence = input(\"Enter place of occurrence: \")\n",
    "    date_of_occurrence = input(\"Enter date and hour of occurrence: \")\n",
    "    offence_nature = input(\"Enter nature of the offence: \")\n",
    "\n",
    "    \n",
    "    complaint_description = input(\"Enter the complaint description: \")\n",
    "    suggested_section = suggest_sections(complaint_description, nlp, textcat.labels, data)\n",
    "\n",
    "    property_description = input(\"Enter particulars of the property: \")\n",
    "    accused_description = input(\"Enter description of the accused: \")\n",
    "    witness_details = input(\"Enter details of witnesses (if any): \")\n",
    "    complaint = input(\"Enter complaint: \")\n",
    "\n",
    "    \n",
    "    fir_template = f\"\"\"Police Station: [Police Station]\n",
    "District: [District]\n",
    "\n",
    "1. Personal details of the Complainant / Informant:\n",
    "(a) Name: {complainant_name}\n",
    "(b) Father's / Husband's Name: {father_name}\n",
    "(c) Address: {address}\n",
    "(d) Phone number & Fax: {phone_number}\n",
    "(e) Email: {email}\n",
    "\n",
    "2. Place of Occurrence: {place_of_occurrence}\n",
    "\n",
    "3. Date and Hour of Occurrence: {date_of_occurrence}\n",
    "\n",
    "4. Offence:\n",
    "(a) Nature of the offence: {offence_nature}\n",
    "(b) Section: {suggested_section}  # Use the suggested section here\n",
    "(c) Particulars of the property: {property_description}\n",
    "\n",
    "5. Description of the accused: {accused_description}  # Use the accused description here\n",
    "\n",
    "6. Details of witnesses (if any): {witness_details}\n",
    "\n",
    "7. Complaint: {complaint}\n",
    "\"\"\"\n",
    "\n",
    "    # Return the generated FIR\n",
    "    return fir_template\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115048/536671001.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = nlp_model(processed_text).similarity(nlp_model(row_text))\n",
      "/tmp/ipykernel_115048/536671001.py:9: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = nlp_model(processed_text).similarity(nlp_model(row_text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Police Station: [Police Station]\n",
      "District: [District]\n",
      "\n",
      "1. Personal details of the Complainant / Informant:\n",
      "(a) Name: Dushyant\n",
      "(b) Father's / Husband's Name: Sanjay\n",
      "(c) Address: nhibtaunga\n",
      "(d) Phone number & Fax: ye bhi nhi btaunga\n",
      "(e) Email: na\n",
      "\n",
      "2. Place of Occurrence: NIT\n",
      "\n",
      "3. Date and Hour of Occurrence: 11:11\n",
      "\n",
      "4. Offence:\n",
      "(a) Nature of the offence: Murder\n",
      "(b) Section: 140  # Use the suggested section here\n",
      "(c) Particulars of the property: NIT\n",
      "\n",
      "5. Description of the accused: reamesh killed suresh  # Use the accused description here\n",
      "\n",
      "6. Details of witnesses (if any): ramesh killed suresh\n",
      "\n",
      "7. Complaint: ramesh killed suresh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "fir_text = generate_fir()\n",
    "print(fir_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
