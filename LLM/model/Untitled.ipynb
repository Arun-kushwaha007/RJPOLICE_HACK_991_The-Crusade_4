{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b2f7b2d-d4ef-4128-b2de-16639644e17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, TFBertForSequenceClassification, AutoModelForTokenClassification, AutoTokenizer, RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoModelForTokenClassification, AutoTokenizer, RagTokenizer, RagRetriever, RagSequenceForGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994f05c9-9361-493f-a3ae-9dd2c6bf4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FIR_DATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2906656a-128b-4891-a696-a7503070d285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "      <th>Offense</th>\n",
       "      <th>Punishment</th>\n",
       "      <th>Cognizable</th>\n",
       "      <th>Bailable</th>\n",
       "      <th>Court</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://lawrato.com/indian-kanoon/ipc/section-140</td>\n",
       "      <td>Description of IPC Section 140\\nAccording to s...</td>\n",
       "      <td>Wearing the dress or carrying any token used b...</td>\n",
       "      <td>3 Months or Fine or Both</td>\n",
       "      <td>Cognizable</td>\n",
       "      <td>Bailable</td>\n",
       "      <td>Any Magistrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://lawrato.com/indian-kanoon/ipc/section-127</td>\n",
       "      <td>Description of IPC Section 127\\nAccording to s...</td>\n",
       "      <td>Receiving property taken by war or depredation...</td>\n",
       "      <td>7 Years + Fine + forfeiture of property</td>\n",
       "      <td>Cognizable</td>\n",
       "      <td>Non-Bailable</td>\n",
       "      <td>Court of Session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://lawrato.com/indian-kanoon/ipc/section-128</td>\n",
       "      <td>Description of IPC Section 128\\nAccording to s...</td>\n",
       "      <td>Public servant voluntarily allowing prisoner o...</td>\n",
       "      <td>Imprisonment for Life or 10 Years + Fine</td>\n",
       "      <td>Cognizable</td>\n",
       "      <td>Non-Bailable</td>\n",
       "      <td>Court of Session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://lawrato.com/indian-kanoon/ipc/section-129</td>\n",
       "      <td>Description of IPC Section 129\\nAccording to s...</td>\n",
       "      <td>Public servant negligently suffering prisoner ...</td>\n",
       "      <td>Simple Imprisonment 3 Years + Fine</td>\n",
       "      <td>Cognizable</td>\n",
       "      <td>Bailable</td>\n",
       "      <td>Magistrate First Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://lawrato.com/indian-kanoon/ipc/section-130</td>\n",
       "      <td>Description of IPC Section 130\\nAccording to s...</td>\n",
       "      <td>Aiding escape of, rescuing or harbouring, such...</td>\n",
       "      <td>Imprisonment for Life or 10 Years + Fine</td>\n",
       "      <td>Cognizable</td>\n",
       "      <td>Non-Bailable</td>\n",
       "      <td>Court of Session</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://lawrato.com/indian-kanoon/ipc/section-140   \n",
       "1  https://lawrato.com/indian-kanoon/ipc/section-127   \n",
       "2  https://lawrato.com/indian-kanoon/ipc/section-128   \n",
       "3  https://lawrato.com/indian-kanoon/ipc/section-129   \n",
       "4  https://lawrato.com/indian-kanoon/ipc/section-130   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Description of IPC Section 140\\nAccording to s...   \n",
       "1  Description of IPC Section 127\\nAccording to s...   \n",
       "2  Description of IPC Section 128\\nAccording to s...   \n",
       "3  Description of IPC Section 129\\nAccording to s...   \n",
       "4  Description of IPC Section 130\\nAccording to s...   \n",
       "\n",
       "                                             Offense  \\\n",
       "0  Wearing the dress or carrying any token used b...   \n",
       "1  Receiving property taken by war or depredation...   \n",
       "2  Public servant voluntarily allowing prisoner o...   \n",
       "3  Public servant negligently suffering prisoner ...   \n",
       "4  Aiding escape of, rescuing or harbouring, such...   \n",
       "\n",
       "                                 Punishment  Cognizable      Bailable  \\\n",
       "0                  3 Months or Fine or Both  Cognizable      Bailable   \n",
       "1   7 Years + Fine + forfeiture of property  Cognizable  Non-Bailable   \n",
       "2  Imprisonment for Life or 10 Years + Fine  Cognizable  Non-Bailable   \n",
       "3        Simple Imprisonment 3 Years + Fine  Cognizable      Bailable   \n",
       "4  Imprisonment for Life or 10 Years + Fine  Cognizable  Non-Bailable   \n",
       "\n",
       "                    Court  \n",
       "0          Any Magistrate  \n",
       "1        Court of Session  \n",
       "2        Court of Session  \n",
       "3  Magistrate First Class  \n",
       "4        Court of Session  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fad41f5-067e-41f5-83a2-9afd6cd17b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_data = df['Description']\n",
    "offense_data = df['Description']\n",
    "offense_labels = df['Offense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b1d9d32-ddd1-4dc0-a5fd-36f37865cf88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data, test_data, train_labels, test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(\n\u001b[0;32m      2\u001b[0m     section_data, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    section_data, df['section'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aac8a7cd-03bf-4f41-85c5-f67307e5c0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e66895383a4a4daf3138d8a4f76df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rebel Soul\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rebel Soul\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c82731f2e9c4a50aa08fe7f2909251d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4881818c39d4d71b52ee1a34aacaee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9a4cc902404228a03924e57ca0ec60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nTFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m bert_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m bert_tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name)\n\u001b[1;32m----> 3\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m \u001b[43mTFBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(bert_model_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1288\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1288\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1276\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1274\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nTFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d96d2-a04d-4a91-9cb3-c58777c04e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_data), truncation=True, padding=True, return_tensors='tf')\n",
    "test_encodings = tokenizer(list(test_data), truncation=True, padding=True, return_tensors='tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d44fe-5185-4e60-93d7-248283295413",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_bert = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "bert_model.compile(optimizer=optimizer_bert, loss=bert_model.compute_loss, metrics=['accuracy'])\n",
    "labels_bert = tf.keras.utils.to_categorical(train_labels, num_classes=len(df['section'].unique()))\n",
    "history_bert = bert_model.fit(train_encodings, labels_bert, epochs=3, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905323b-c81e-4c6f-804c-985d8768078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.evaluate(test_encodings, tf.keras.utils.to_categorical(test_labels, num_classes=len(df['section'].unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d5452-821b-4862-be49-a1821bbdeb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_model_path = 'path_to_llama2_model'  # Replace with the actual path to your LLama-2 model files\n",
    "tokenizer_llama2 = AutoTokenizer.from_pretrained(llama2_model_path)\n",
    "model_llama2 = AutoModelForTokenClassification.from_pretrained(llama2_model_path)\n",
    "\n",
    "llama2_encodings = tokenizer_llama2(list(offense_data), truncation=True, padding=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361211c7-2129-4471-b337-43c9c8db5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_llama2 = torch.optim.AdamW(model_llama2.parameters(), lr=5e-5)\n",
    "labels_llama2 = torch.tensor(list(offense_labels))  # Assuming you have labels for LLama-2\n",
    "optimizer_llama2.zero_grad()\n",
    "outputs_llama2 = model_llama2(**llama2_encodings, labels=labels_llama2)\n",
    "loss_llama2 = outputs_llama2.loss\n",
    "loss_llama2.backward()\n",
    "optimizer_llama2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0957c3d-397e-4bd0-bade-3c8ebe864462",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama2.eval()\n",
    "with torch.no_grad():\n",
    "    logits_llama2 = model_llama2(**llama2_encodings).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccaf17-390c-4995-abfb-29d8b7389bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_bert = bert_model.predict(test_encodings)\n",
    "predicted_labels_bert = tf.argmax(predicted_labels_bert['logits'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66d0eb-fd71-41bf-9bba-405e2067389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_llama2 = torch.argmax(logits_llama2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff52c95f-f3cd-408e-b6fb-9db1248136ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RagTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rag_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRagTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m rag_retriever \u001b[38;5;241m=\u001b[39m RagRetriever\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m rag_model \u001b[38;5;241m=\u001b[39m RagSequenceForGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RagTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "rag_retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b607742-e43f-4163-a95c-e3d9bcff4893",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(offense_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Use a sample context\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mrag_tokenizer\u001b[49m(context, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rag_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "context = \" \".join(offense_data.iloc[0:5]) \n",
    "input_ids = rag_tokenizer(context, return_tensors=\"pt\").input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7803f01c-f649-4b37-87e0-38eedb959c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m \u001b[43mrag_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(input_ids)\n\u001b[0;32m      2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m rag_tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rag_model' is not defined"
     ]
    }
   ],
   "source": [
    "generated_output = rag_model.generate(input_ids)\n",
    "generated_text = rag_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d8b04-b924-489d-bc13-01133b964cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
